import json
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import requests

app = FastAPI()

OLLAMA_API_URL = "http://localhost:11434/api/generate"

class QueryModel(BaseModel):
    query: str

# ‚úÖ Ajustamos la funci√≥n de streaming para mejor compatibilidad
def stream_ollama_response(query: str):
    payload = {
        "model": "deepseek-r1:8b",
        "prompt": query,
        "stream": True
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è Error al conectar con Ollama: {e}")
        raise HTTPException(status_code=500, detail=f"Error conectando a Ollama: {e}")

    buffer = ""  # Acumulador de texto

    def event_generator():
        nonlocal buffer
        for line in response.iter_lines():
            if line:
                data = json.loads(line.decode("utf-8"))
                chunk = data.get("response", "")

                buffer += chunk  # üîπ Acumular los fragmentos en lugar de enviarlos incompletos

                if "." in chunk or "\n" in chunk:  # üîπ Enviar solo cuando hay punto o salto de l√≠nea
                    yield buffer + "\n"
                    buffer = ""  # üîπ Reiniciar el buffer

    return event_generator()

@app.post("/ask_stream/")
async def ask_stream(payload: QueryModel, request: Request):
    print(f"üì° Recibida pregunta: {payload.query}")

    return StreamingResponse(
        stream_ollama_response(payload.query), 
        media_type="text/event-stream",  # ‚úÖ Cambiado a text/event-stream para mejor compatibilidad
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Transfer-Encoding": "chunked"
        }
    )